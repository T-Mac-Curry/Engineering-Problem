### 问1：通过堆叠神经网络层数（增加深度）可以非常有效地增强表征，提升特征学习效果。那么为什么网络的深度很重要  
答：因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富，增强网络表征能力。并且，越深的网络提取的特征越抽象，越具有语义信息。

# 问2：如何解决梯度爆炸/梯度消失等问题。
答：normalized initialization  和  batch normalization等措施

# 问3：什么叫identity mapping？
答：在原始的浅层网络基础上增加的层视为identity mapping(恒等映射)

# 问4：网络越深越好么？
答：直觉上深度网络应该会有更好的表征能力。但是事实却是深度网络结果会变差，由此我们认为深度网络的优化部分出现了问题，深度网络的参数空间变得更复杂提升了优化的难度。
所以resnet来了

# 问5：resnet网络是如何提出来的？
答：深度网络越深，结果反而不好了，优化部分出现了问题。

# 问6：resnet基础思想？
答：与其拟合一个desired underlying mapping（所需基础映射）H(x)，不如让网络尝试拟合一个residual mapping（残差图）F(x). 
    即：F(x) = H(x) - x
    原先的映射H(x), 被转换为了F(x) + x，网络结构上，可以使用短路链接（shortcut connection）结构来实现。
    假设：F(x)是比优化原映射H(x)要容易的。
![image](https://github.com/T-Mac-Curry/Engineering-Problem/blob/master/images/resnet1.jpg)
    
# 问7：什么是shortcut？
答：设计的可以skip几层的结构，在resnet中就是起到了相当于一个十分简单的identity mapping，其输出被加到了stacked layers的输出上。这样做既没有增加新的参数，也没有增加计算复杂性。

# 问8：残差网络解决什么问题？
答：为了解决神经网络层过多时的网络退化问题而提出。

# 问9：什么是退化（degradation）问题
答：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。

# 问10：问什么残差函数是F(x) = H(x) - x？
答：如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。
    那现在要解决的就是学习恒等映射函数了。 
    但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。
    但是，如果把网络设计为H(x) = F(x) + x,如下图。我们可以转换为学习一个残差函数F(x) = H(x) - x. 
    只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。
    

