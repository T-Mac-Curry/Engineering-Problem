#### 问1：通过堆叠神经网络层数（增加深度）可以非常有效地增强表征，提升特征学习效果。那么为什么网络的深度很重要  
答：因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富，增强网络表征能力。并且，越深的网络提取的特征越抽象，越具有语义信息。

#### 续1：为什么不能简单的增加网络层数？
答：简单地增加深度，会导致梯度弥散或梯度爆炸

#### 问2：如何解决梯度爆炸/梯度消失等问题。
答：normalized initialization  和  batch normalization等措施

#### 问3：什么叫identity mapping？
答：在原始的浅层网络基础上增加的层视为identity mapping(恒等映射)

#### 问4：网络越深越好么？
答：直觉上深度网络应该会有更好的表征能力。但是事实却是深度网络结果会变差，由此我们认为深度网络的优化部分出现了问题，深度网络的参数空间变得更复杂提升了优化的难度。
所以resnet来了

#### 问5：resnet网络是如何提出来的？
答：深度网络越深，结果反而不好了，优化部分出现了问题。

#### 问6：resnet基础思想？
答：拟合潜在的恒等映射函数H(x) = x。但是比较困难，可能是深层网络难以训练的原因。
    如果把网络设计为H(x) = F(x) + x， 转换为学习一个残差函数F(x) = H(x) - x. 只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。
    与其拟合一个desired underlying mapping（所需基础映射）H(x)，不如让网络尝试拟合一个residual mapping（残差图）F(x). 
    即：F(x) = H(x) - x
    原先的映射H(x), 被转换为了F(x) + x，网络结构上，可以使用短路链接（shortcut connection）结构来实现。
    假设：F(x)是比优化原映射H(x)要容易的。
![image](https://github.com/T-Mac-Curry/Engineering-Problem/blob/master/images/resnet1.jpg)

#### 问7：为什么残差函数是F(x) = H(x) - x？
答：
    如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那么在训练的前向和反向传播阶段，信号可以直接从一个单元传递到另外一个单元，使训练变得更加简单。
    那现在要解决的就是学习恒等映射函数了。 
    但是直接让一些层去拟合一个潜在的恒等映射函数H(x) = x，比较困难，这可能就是深层网络难以训练的原因。
    但是，如果把网络设计为H(x) = F(x) + x,如下图。我们可以转换为学习一个残差函数F(x) = H(x) - x. 
    只要F(x)=0，就构成了一个恒等映射H(x) = x. 而且，拟合残差肯定更加容易。
    
#### 问8：什么是shortcut？
答：设计的可以skip几层的结构，在resnet中就是起到了相当于一个十分简单的identity mapping，其输出被加到了stacked layers的输出上。这样做既没有增加新的参数，也没有增加计算复杂性。

#### 问9：残差网络解决什么问题？
答：为了解决神经网络层过多时的网络退化问题而提出。

#### 问10：什么是退化（degradation）问题
答：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。

#### 续10：如何理解退化和过拟合问题
答：随着网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。

#### 问11：怎么解决退化问题
答：Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。
    通俗答法：F是求和前网络映射，H是从输入到求和后的网络映射。
    比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。
    这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。
    比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。
    明显后者输出变化对权重的调整作用更大，所以效果更好。
    残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器。

#### 问12：残差的思想是什么？
答：残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。 去掉相同的主体部分，从而突出微小的变化。


    

