问1：通过堆叠神经网络层数（增加深度）可以非常有效地增强表征，提升特征学习效果。那么为什么深度的网络表征效果会好？
答：深度学习很不好解释，大概的解释可以是：网络的不同层可以提取不同抽象层次的特征，越深的层提取的特征越抽象。因此深度网络可以整合low-medium-high各种层次的特征，增强网络表征能力。

问2：如何解决梯度爆炸/梯度消失等问题。
答：normalized initialization  和  batch normalization等措施

问3：什么叫identity mapping？
答：在原始的浅层网络基础上增加的层视为identity mapping

问4：网络越深越好么？
答：直觉上深度网络应该会有更好的表征能力。但是事实却是深度网络结果会变差，由此我们认为深度网络的优化部分出现了问题，深度网络的参数空间变得更复杂提升了优化的难度。
所以resnet来了

问5：resnet网络是如何提出来的？
答：深度网络越深，结果反而不好了，优化部分出现了问题。

问6：resnet基础思想？
答：与其拟合一个desired underlying mapping（所需基础映射）H(x)，不如让网络尝试拟合一个residual mapping（残差图）F(x). 
    即：F(x) = H(x) - x
    原先的映射H(x), 被转换为了F(x) + x
    假设：F(x)是比优化原映射H(x)要容易的。
![image](https://github.com/T-Mac-Curry/Engineering-Problem/blob/master/images/resnet1.jpg?raw=true)

    
